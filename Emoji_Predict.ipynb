{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWT_bWi3wOkk"
      },
      "source": [
        "# Installation: Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv24dloAvjB4"
      },
      "outputs": [],
      "source": [
        "# Download Japanese Word Vector\n",
        "!wget -c \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\"\n",
        "!gzip -d cc.ja.300.vec.gz\n",
        "\n",
        "# Install FastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText && pip install .\n",
        "\n",
        "# Install MeCab\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "\n",
        "# Download the Dataset\n",
        "!wget https://github.com/SysWOW64UwU/Emoji_Predict/raw/main/SomethingSomething.zip\n",
        "!unzip -P \"æ…°ç³æ½·æ‘²\" SomethingSomething.zip\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrAthXQvy5y"
      },
      "source": [
        "# Training and Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i7wcE-M7v08N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import MeCab\n",
        "import gensim\n",
        "\n",
        "# Load the dataset\n",
        "with open('dataset.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Define the maximum length of the input text\n",
        "MAX_LENGTH = max(set([len(entry['text'])for entry in dataset['data']]))\n",
        "\n",
        "# Define the path to the pre-trained Japanese word embeddings\n",
        "EMBEDDINGS_PATH = 'cc.ja.300.vec'\n",
        "\n",
        "# Load the pre-trained Japanese word embeddings\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDINGS_PATH, binary=False, encoding='utf-8')\n",
        "\n",
        "# Define the vocabulary size\n",
        "VOCAB_SIZE = len(embeddings.key_to_index)\n",
        "\n",
        "# Define the dimensionality of the word embeddings\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Define the number of unique emojis in the dataset\n",
        "NUM_EMOJIS = len(set([entry['emoji'] for entry in dataset['data']]))\n",
        "\n",
        "# Define the MeCab tokenizer\n",
        "tokenizer = MeCab.Tagger('-Owakati')\n",
        "\n",
        "# Tokenize the input text\n",
        "def tokenize(text):\n",
        "    return tokenizer.parse(text).split()\n",
        "\n",
        "# Pad or truncate the input text to the maximum length\n",
        "def pad_or_truncate(tokens):\n",
        "    if len(tokens) > MAX_LENGTH:\n",
        "        return tokens[:MAX_LENGTH]\n",
        "    else:\n",
        "        return tokens + [''] * (MAX_LENGTH - len(tokens))\n",
        "\n",
        "# Convert the input text to a sequence of word embeddings\n",
        "def text_to_sequence(text):\n",
        "    tokens = tokenize(text)\n",
        "    padded_tokens = pad_or_truncate(tokens)\n",
        "    sequence = np.zeros((MAX_LENGTH, EMBEDDING_DIM))\n",
        "    for i, token in enumerate(padded_tokens):\n",
        "        if token in embeddings.key_to_index:\n",
        "            sequence[i] = embeddings.get_vector(token)\n",
        "    return sequence\n",
        "\n",
        "# Convert the input emoji to a one-hot vector\n",
        "def emoji_to_one_hot(emoji):\n",
        "    one_hot = np.zeros(NUM_EMOJIS)\n",
        "    for i, unique_emoji in enumerate(set([entry['emoji'] for entry in dataset['data']])):\n",
        "        if emoji == unique_emoji:\n",
        "            one_hot[i] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_LENGTH, EMBEDDING_DIM)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(NUM_EMOJIS, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the training data\n",
        "X_train = np.array([text_to_sequence(entry['text']) for entry in dataset['data']])\n",
        "y_train = np.array([emoji_to_one_hot(entry['emoji']) for entry in dataset['data']])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Save the model\n",
        "model.save('emoji_translator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p79nBO-MxXcX"
      },
      "source": [
        "# Load the Saved Model and Define a Function for Predicting Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4DfzpALLwznB"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = 'emoji_translator.h5'\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "# Define a function that takes an input text and returns the predicted emoji\n",
        "def predict_emoji(text):\n",
        "    sequence = text_to_sequence(text)\n",
        "    prediction = model.predict(np.array([sequence]))\n",
        "    index = np.argmax(prediction)\n",
        "    emojis = list(set([entry['emoji'] for entry in dataset['data']]))\n",
        "    return emojis[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG9SszYS8KDl"
      },
      "source": [
        "# Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFrQiG4A8JZM"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    'ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­',\n",
        "    'æ˜æ—¥ã¯è©¦é¨“ãŒã‚ã‚Šã¾ã™',\n",
        "    'æ˜¨æ—¥ã¯ã¨ã¦ã‚‚æ‚²ã—ã„å‡ºæ¥äº‹ãŒã‚ã‚Šã¾ã—ãŸ',\n",
        "    'ä»Šæ™©ã¯ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã«è¡Œãã¾ã™',\n",
        "    'æœ€è¿‘ã¯å¿™ã—ãã¦ã‚¹ãƒˆãƒ¬ã‚¹ãŒãŸã¾ã£ã¦ã„ã¾ã™',\n",
        "    'æ—…è¡Œã«è¡ŒããŸã„ã§ã™',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚ç–²ã‚Œã¦ã„ã¾ã™',\n",
        "    'å‹é”ã¨ä¸€ç·’ã«æ˜ ç”»ã‚’è¦‹ã«è¡Œãã¾ã™',\n",
        "    'æ–°ã—ã„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã«è¡Œã£ã¦ã¿ãŸã„ã§ã™',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚æš‘ã„ã§ã™',\n",
        "    'æ˜æ—¥ã‹ã‚‰æ–°ã—ã„ä»•äº‹ãŒå§‹ã¾ã‚Šã¾ã™',\n",
        "    'å‹é”ã¨ä¸€ç·’ã«é‹å‹•ã‚’ã—ã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚å¹¸ã›ãªæ°—åˆ†ã§ã™',\n",
        "    'æ–°ã—ã„è¶£å‘³ã‚’å§‹ã‚ã¾ã—ãŸ',\n",
        "    'æœ€è¿‘ã¯é£Ÿã¹éãã¦å¤ªã£ã¦ã—ã¾ã„ã¾ã—ãŸ',\n",
        "    'æ˜¨æ—¥ã¯ã¨ã¦ã‚‚æ¥½ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆã«å‚åŠ ã—ã¾ã—ãŸ',\n",
        "    'ä»Šé€±æœ«ã¯å®¶æ—ã¨æ—…è¡Œã«è¡Œãäºˆå®šã§ã™',\n",
        "    'æ˜æ—¥ã¯å¤§äº‹ãªãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™',\n",
        "    'å‹é”ã¨ä¸€ç·’ã«ã‚«ãƒ©ã‚ªã‚±ã«è¡Œãã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚å¿™ã—ã„æ—¥ã§ã—ãŸ',\n",
        "    'æ˜æ—¥ã‹ã‚‰æ—…è¡Œã«è¡Œãäºˆå®šã§ã™',\n",
        "    'æœ€è¿‘ã¯å¯’ãã¦ã‚³ãƒ¼ãƒˆãŒæ‰‹æ”¾ã›ã¾ã›ã‚“',\n",
        "    'æ–°ã—ã„æœ¬ã‚’èª­ã¿å§‹ã‚ã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚å¿«æ™´ã§æ°—æŒã¡ãŒã„ã„ã§ã™',\n",
        "    'å‹é”ã¨ä¸€ç·’ã«æ–™ç†ã‚’ä½œã‚Šã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚æš‡ã§ä½•ã‚’ã—ã‚ˆã†ã‹è¿·ã£ã¦ã„ã¾ã™',\n",
        "    'æ–°ã—ã„æ˜ ç”»ãŒå…¬é–‹ã•ã‚Œã‚‹ã®ã‚’æ¥½ã—ã¿ã«ã—ã¦ã„ã¾ã™',\n",
        "    'æœ€è¿‘ã¯é‹å‹•ä¸è¶³ã§ä½“ãŒé‡ã„ã§ã™',\n",
        "    'æ˜¨æ—¥ã¯ã¨ã¦ã‚‚çœ ã‹ã£ãŸã®ã§æ—©ãå¯ã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯å®¶ã§ã‚†ã£ãã‚Šéã”ã™ã¤ã‚‚ã‚Šã§ã™',\n",
        "    'æ–°ã—ã„éŸ³æ¥½ã‚’è´ãå§‹ã‚ã¾ã—ãŸ',\n",
        "    'æ˜æ—¥ã®å¤©æ°—äºˆå ±ã¯é›¨ãŒé™ã‚‹ãã†ã§ã™',\n",
        "    'ä»Šé€±æœ«ã¯å‹é”ã¨éŠåœ’åœ°ã«è¡Œãäºˆå®šã§ã™',\n",
        "    'æœ€è¿‘ã¯å¿™ã—ãã¦å‹é”ã¨ä¼šã†æ™‚é–“ãŒãªã‹ãªã‹å–ã‚Œã¾ã›ã‚“',\n",
        "    'æ˜¨æ—¥ã¯ã¨ã¦ã‚‚æš‘ãã¦å¤–ã«å‡ºã‚‹ã®ãŒè¾›ã‹ã£ãŸã§ã™',\n",
        "    'ä»Šæ—¥ã¯ä»•äº‹ãŒä¼‘ã¿ãªã®ã§ã‚†ã£ãã‚Šå¯åŠã—ã¾ã—ãŸ',\n",
        "    'æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã«æŒ‘æˆ¦ã—ã¦ã¿ãŸã„ã§ã™',\n",
        "    'æœ€è¿‘ã¯ã‚¹ãƒãƒ›ã‚’ä½¿ã„ã™ãã¦ç›®ãŒç–²ã‚Œã¦ãã¾ã—ãŸ',\n",
        "    'æ˜æ—¥ã¯å‹é”ã®èª•ç”Ÿæ—¥ãªã®ã§ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚’è²·ã„ã«è¡Œãã¾ã™',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚æ‚²ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯å®¶æ—ã¨ä¸€ç·’ã«ãƒ”ã‚¯ãƒ‹ãƒƒã‚¯ã«è¡Œãäºˆå®šã§ã™',\n",
        "    'æœ€è¿‘ã¯ä»•äº‹ãŒå¿™ã—ãã¦ã‚¹ãƒˆãƒ¬ã‚¹ãŒãŸã¾ã£ã¦ã„ã¾ã™',\n",
        "    'æ–°ã—ã„ã‚¹ãƒãƒ¼ãƒ„ã‚’å§‹ã‚ã¦ã¿ãŸã„ã§ã™',\n",
        "    'æ˜¨æ—¥ã¯ã¨ã¦ã‚‚æ¥½ã—ã‹ã£ãŸã§ã™',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚å¯’ã„ã®ã§ãŠã†ã¡ã§ã¾ã£ãŸã‚Šã—ã¦ã„ã¾ã™',\n",
        "    'æ–°ã—ã„æ˜ ç”»ã‚’è¦‹ã‚‹ã®ãŒæ¥½ã—ã¿ã§ã™',\n",
        "    'ä»Šæ—¥ã¯ä½•ã‹ç‰¹åˆ¥ãªã“ã¨ã‚’ã—ãŸã„æ°—åˆ†ã§ã™',\n",
        "    'æœ€è¿‘ã¯é£Ÿäº‹ã®ãƒãƒ©ãƒ³ã‚¹ãŒæ‚ªãã¦ä½“èª¿ãŒã™ãã‚Œã¾ã›ã‚“',\n",
        "    'æ˜¨æ—¥ã¯å‹é”ã¨ãŠã—ã‚ƒã¹ã‚Šã™ã‚‹ã®ã«å¤¢ä¸­ã«ãªã‚Šã¾ã—ãŸ',\n",
        "    'ä»Šæ™©ã¯å®¶ã§ã‚†ã£ãã‚Šã—ã¦éã”ã™ã¤ã‚‚ã‚Šã§ã™',\n",
        "    'æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ',\n",
        "    'æ˜æ—¥ã‹ã‚‰æ–°ã—ã„ãƒ€ã‚¤ã‚¨ãƒƒãƒˆã‚’å§‹ã‚ã¾ã™',\n",
        "    'æœ€è¿‘ã¯æ˜ ç”»ã‚’ã‚ˆãè¦‹ã¾ã™',\n",
        "    'ä»Šé€±æœ«ã¯å®¶æ—ã§æ¸©æ³‰æ—…è¡Œã«è¡Œãäºˆå®šã§ã™',\n",
        "    'æ˜¨æ—¥ã¯é›¨ãŒé™ã£ã¦ã„ãŸã®ã§å®¶ã§éã”ã—ã¾ã—ãŸ',\n",
        "    'ä»Šæ—¥ã¯ä½•ã‹ç¾å‘³ã—ã„ã‚‚ã®ã‚’é£Ÿã¹ãŸã„æ°—åˆ†ã§ã™',\n",
        "    'æœ€è¿‘ã¯æœ¬ã‚’èª­ã‚€æ™‚é–“ãŒãªã‹ãªã‹å–ã‚Œã¾ã›ã‚“',\n",
        "    'æ˜æ—¥ã¯ã¨ã¦ã‚‚å¤§åˆ‡ãªãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™',\n",
        "    'ä»Šæ—¥ã¯ã¨ã¦ã‚‚çœ ã„ã®ã§ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚“ã§ç›®ã‚’è¦šã¾ã—ã¾ã™',\n",
        "    'æ–°ã—ã„è¶£å‘³ã‚’è¦‹ã¤ã‘ãŸã„ã§ã™',\n",
        "]\n",
        "\n",
        "expected_emojis = [\n",
        "    'â˜€ï¸',\n",
        "    'ğŸ“š',\n",
        "    'ğŸ˜¢',\n",
        "    'ğŸ‰',\n",
        "    'ğŸ˜«',\n",
        "    'âœˆï¸',\n",
        "    'ğŸ˜´',\n",
        "    'ğŸ¥',\n",
        "    'ğŸ´',\n",
        "    'ğŸ”¥',\n",
        "    'ğŸ’¼',\n",
        "    'ğŸ‹ï¸â€â™‚ï¸',\n",
        "    'ğŸ˜Š',\n",
        "    'ğŸ¨',\n",
        "    'ğŸ·',\n",
        "    'ğŸŠ',\n",
        "    'ğŸš—',\n",
        "    'ğŸ’¼',\n",
        "    'ğŸ¤',\n",
        "    'ğŸ’»',\n",
        "    'âœˆï¸',\n",
        "    'ğŸ§¥',\n",
        "    'ğŸ“–',\n",
        "    'â˜€ï¸',\n",
        "    'ğŸ‘©â€ğŸ³',\n",
        "    'ğŸ¤”',\n",
        "    'ğŸ¬',\n",
        "    'ğŸ‹ï¸â€â™‚ï¸',\n",
        "    'ğŸ’¤',\n",
        "    'ğŸ ',\n",
        "    'ğŸ¶',\n",
        "    'â˜”',\n",
        "    'ğŸ¢',\n",
        "    'ğŸ˜”',\n",
        "    'ğŸ”¥',\n",
        "    'ğŸ˜´',\n",
        "    'ğŸ´',\n",
        "    'ğŸ˜ª',\n",
        "    'ğŸ',\n",
        "    'ğŸ˜¢',\n",
        "    'ğŸ§º',\n",
        "    'ğŸ˜«',\n",
        "    'ğŸ€',\n",
        "    'ğŸ˜„',\n",
        "    'ğŸ â„ï¸',\n",
        "    'ğŸ¿',\n",
        "    'ğŸ‰',\n",
        "    'ğŸ',\n",
        "    'ğŸ’¬',\n",
        "    'ğŸ ',\n",
        "    'ğŸ“±',\n",
        "    'ğŸ¥—',\n",
        "    'ğŸ¥',\n",
        "    'â™¨ï¸',\n",
        "    'ğŸŒ§ï¸',\n",
        "    'ğŸ½ï¸',\n",
        "    'ğŸ“š',\n",
        "    'ğŸ‘”ğŸ’¼',\n",
        "    'ğŸ˜´â˜•ï¸',\n",
        "    'ğŸ¨',\n",
        "]\n",
        "\n",
        "test_translations = [\n",
        "    'Today is good weather, isn\\'t it?',\n",
        "    'There is an exam tomorrow.',\n",
        "    'There was a very sad incident yesterday.',\n",
        "    'I\\'m going to a party tonight.',\n",
        "    'I\\'ve been busy lately and stressed out.',\n",
        "    'I want to go on a trip.',\n",
        "    'I\\'m very tired today.',\n",
        "    'I\\'m going to see a movie with my friend.',\n",
        "    'I want to go to a new restaurant.',\n",
        "    'It\\'s very hot today.',\n",
        "    'I\\'m starting a new job from tomorrow.',\n",
        "    'I exercised with my friend.',\n",
        "    'I\\'m very happy today.',\n",
        "    'I started a new hobby.',\n",
        "    'I\\'ve been eating too much and gained weight.',\n",
        "    'I participated in a very fun event yesterday.',\n",
        "    'I\\'m planning to go on a trip with my family this weekend.',\n",
        "    'There is an important presentation tomorrow.',\n",
        "    'I went to karaoke with my friend.',\n",
        "    'Today was a very busy day.',\n",
        "    'I am planning to go on a trip from tomorrow.',\n",
        "    'I\\'ve been feeling cold lately and can\\'t let go of my coat.',\n",
        "    'I started reading a new book.',\n",
        "    'Today is very clear and feels good.',\n",
        "    'I cooked with my friend.',\n",
        "    'I\\'m very bored today and don\\'t know what to do.',\n",
        "    'I\\'m looking forward to the release of a new movie.',\n",
        "    'I\\'ve been inactive lately and my body feels heavy.',\n",
        "    'I slept early yesterday because I was very sleepy.',\n",
        "    'I plan to spend a relaxing day at home today.',\n",
        "    'I started listening to new music.',\n",
        "    'The weather forecast for tomorrow says it will rain.',\n",
        "    'I plan to go to an amusement park with my friends this weekend.',\n",
        "    'I\\'ve been very busy lately and haven\\'t had time to meet my friends.',\n",
        "    'Yesterday was very hot and it was hard to go outside.',\n",
        "    'I slept in today because I had a day off from work.',\n",
        "    'I want to try a new recipe.',\n",
        "    'I\\'ve been using my smartphone too much lately and my eyes are getting tired.',\n",
        "    'I will go to buy a present for my friend\\'s birthday tomorrow.',\n",
        "    'There was very sad news today.',\n",
        "    'I am planning to go on a picnic with my family today.',\n",
        "    'I have been busy with work lately and have been accumulating stress.',\n",
        "    'I want to try starting a new sport.',\n",
        "    'Yesterday was a lot of fun.',\n",
        "    'It is very cold today, so I am relaxing at home.',\n",
        "    'I am looking forward to watching a new movie.',\n",
        "    'I feel like doing something special today.',\n",
        "    'My diet has been unbalanced lately and my health is not good.',\n",
        "    'I got carried away talking with my friend yesterday.',\n",
        "    'I plan to spend a relaxing evening at home tonight.',\n",
        "    'I downloaded a new app.',\n",
        "    'I will start a new diet from tomorrow.',\n",
        "    'I have been watching a lot of movies lately.',\n",
        "    'I plan to go on a hot spring trip with my family this weekend.',\n",
        "    'It was raining yesterday, so I spent time at home.',\n",
        "    'I feel like eating something delicious today.',\n",
        "    'I haven\\'t had time to read books lately.',\n",
        "    'I have a very important presentation tomorrow.',\n",
        "    'I am very sleepy today, so I will drink coffee to wake up.',\n",
        "    'I want to find a new hobby.',\n",
        "]\n",
        "\n",
        "predicted_emojis = []\n",
        "for x in test_texts:predicted_emojis.append(predict_emoji(x))\n",
        "\n",
        "for c,x,y,z in zip(range(len(test_translations)),test_translations, expected_emojis, predicted_emojis):\n",
        "  print(c+1,x,y,z)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
