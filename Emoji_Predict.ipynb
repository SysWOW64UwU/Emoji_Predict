{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWT_bWi3wOkk"
      },
      "source": [
        "# Installation: Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv24dloAvjB4"
      },
      "outputs": [],
      "source": [
        "# Download Japanese Word Vector\n",
        "!wget -c \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ja.300.vec.gz\"\n",
        "!gzip -d cc.ja.300.vec.gz\n",
        "\n",
        "# Install FastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText && pip install .\n",
        "\n",
        "# Install MeCab\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!pip install mecab-python3\n",
        "!pip install unidic-lite\n",
        "\n",
        "# Download the Dataset\n",
        "!wget https://github.com/SysWOW64UwU/Emoji_Predict/raw/main/SomethingSomething.zip\n",
        "!unzip -P \"慰獳潷摲\" SomethingSomething.zip\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgrAthXQvy5y"
      },
      "source": [
        "# Training and Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i7wcE-M7v08N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import MeCab\n",
        "import gensim\n",
        "\n",
        "# Load the dataset\n",
        "with open('dataset.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Define the maximum length of the input text\n",
        "MAX_LENGTH = max(set([len(entry['text'])for entry in dataset['data']]))\n",
        "\n",
        "# Define the path to the pre-trained Japanese word embeddings\n",
        "EMBEDDINGS_PATH = 'cc.ja.300.vec'\n",
        "\n",
        "# Load the pre-trained Japanese word embeddings\n",
        "embeddings = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDINGS_PATH, binary=False, encoding='utf-8')\n",
        "\n",
        "# Define the vocabulary size\n",
        "VOCAB_SIZE = len(embeddings.key_to_index)\n",
        "\n",
        "# Define the dimensionality of the word embeddings\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Define the number of unique emojis in the dataset\n",
        "NUM_EMOJIS = len(set([entry['emoji'] for entry in dataset['data']]))\n",
        "\n",
        "# Define the MeCab tokenizer\n",
        "tokenizer = MeCab.Tagger('-Owakati')\n",
        "\n",
        "# Tokenize the input text\n",
        "def tokenize(text):\n",
        "    return tokenizer.parse(text).split()\n",
        "\n",
        "# Pad or truncate the input text to the maximum length\n",
        "def pad_or_truncate(tokens):\n",
        "    if len(tokens) > MAX_LENGTH:\n",
        "        return tokens[:MAX_LENGTH]\n",
        "    else:\n",
        "        return tokens + [''] * (MAX_LENGTH - len(tokens))\n",
        "\n",
        "# Convert the input text to a sequence of word embeddings\n",
        "def text_to_sequence(text):\n",
        "    tokens = tokenize(text)\n",
        "    padded_tokens = pad_or_truncate(tokens)\n",
        "    sequence = np.zeros((MAX_LENGTH, EMBEDDING_DIM))\n",
        "    for i, token in enumerate(padded_tokens):\n",
        "        if token in embeddings.key_to_index:\n",
        "            sequence[i] = embeddings.get_vector(token)\n",
        "    return sequence\n",
        "\n",
        "# Convert the input emoji to a one-hot vector\n",
        "def emoji_to_one_hot(emoji):\n",
        "    one_hot = np.zeros(NUM_EMOJIS)\n",
        "    for i, unique_emoji in enumerate(set([entry['emoji'] for entry in dataset['data']])):\n",
        "        if emoji == unique_emoji:\n",
        "            one_hot[i] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(MAX_LENGTH, EMBEDDING_DIM)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(NUM_EMOJIS, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Prepare the training data\n",
        "X_train = np.array([text_to_sequence(entry['text']) for entry in dataset['data']])\n",
        "y_train = np.array([emoji_to_one_hot(entry['emoji']) for entry in dataset['data']])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Save the model\n",
        "model.save('emoji_translator.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p79nBO-MxXcX"
      },
      "source": [
        "# Load the Saved Model and Define a Function for Predicting Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4DfzpALLwznB"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = 'emoji_translator.h5'\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "# Define a function that takes an input text and returns the predicted emoji\n",
        "def predict_emoji(text):\n",
        "    sequence = text_to_sequence(text)\n",
        "    prediction = model.predict(np.array([sequence]))\n",
        "    index = np.argmax(prediction)\n",
        "    emojis = list(set([entry['emoji'] for entry in dataset['data']]))\n",
        "    return emojis[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG9SszYS8KDl"
      },
      "source": [
        "# Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFrQiG4A8JZM"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    '今日はいい天気ですね',\n",
        "    '明日は試験があります',\n",
        "    '昨日はとても悲しい出来事がありました',\n",
        "    '今晩はパーティーに行きます',\n",
        "    '最近は忙しくてストレスがたまっています',\n",
        "    '旅行に行きたいです',\n",
        "    '今日はとても疲れています',\n",
        "    '友達と一緒に映画を見に行きます',\n",
        "    '新しいレストランに行ってみたいです',\n",
        "    '今日はとても暑いです',\n",
        "    '明日から新しい仕事が始まります',\n",
        "    '友達と一緒に運動をしました',\n",
        "    '今日はとても幸せな気分です',\n",
        "    '新しい趣味を始めました',\n",
        "    '最近は食べ過ぎて太ってしまいました',\n",
        "    '昨日はとても楽しいイベントに参加しました',\n",
        "    '今週末は家族と旅行に行く予定です',\n",
        "    '明日は大事なプレゼンテーションがあります',\n",
        "    '友達と一緒にカラオケに行きました',\n",
        "    '今日はとても忙しい日でした',\n",
        "    '明日から旅行に行く予定です',\n",
        "    '最近は寒くてコートが手放せません',\n",
        "    '新しい本を読み始めました',\n",
        "    '今日はとても快晴で気持ちがいいです',\n",
        "    '友達と一緒に料理を作りました',\n",
        "    '今日はとても暇で何をしようか迷っています',\n",
        "    '新しい映画が公開されるのを楽しみにしています',\n",
        "    '最近は運動不足で体が重いです',\n",
        "    '昨日はとても眠かったので早く寝ました',\n",
        "    '今日は家でゆっくり過ごすつもりです',\n",
        "    '新しい音楽を聴き始めました',\n",
        "    '明日の天気予報は雨が降るそうです',\n",
        "    '今週末は友達と遊園地に行く予定です',\n",
        "    '最近は忙しくて友達と会う時間がなかなか取れません',\n",
        "    '昨日はとても暑くて外に出るのが辛かったです',\n",
        "    '今日は仕事が休みなのでゆっくり寝坊しました',\n",
        "    '新しいレシピに挑戦してみたいです',\n",
        "    '最近はスマホを使いすぎて目が疲れてきました',\n",
        "    '明日は友達の誕生日なのでプレゼントを買いに行きます',\n",
        "    '今日はとても悲しいニュースがありました',\n",
        "    '今日は家族と一緒にピクニックに行く予定です',\n",
        "    '最近は仕事が忙しくてストレスがたまっています',\n",
        "    '新しいスポーツを始めてみたいです',\n",
        "    '昨日はとても楽しかったです',\n",
        "    '今日はとても寒いのでおうちでまったりしています',\n",
        "    '新しい映画を見るのが楽しみです',\n",
        "    '今日は何か特別なことをしたい気分です',\n",
        "    '最近は食事のバランスが悪くて体調がすぐれません',\n",
        "    '昨日は友達とおしゃべりするのに夢中になりました',\n",
        "    '今晩は家でゆっくりして過ごすつもりです',\n",
        "    '新しいアプリをダウンロードしました',\n",
        "    '明日から新しいダイエットを始めます',\n",
        "    '最近は映画をよく見ます',\n",
        "    '今週末は家族で温泉旅行に行く予定です',\n",
        "    '昨日は雨が降っていたので家で過ごしました',\n",
        "    '今日は何か美味しいものを食べたい気分です',\n",
        "    '最近は本を読む時間がなかなか取れません',\n",
        "    '明日はとても大切なプレゼンテーションがあります',\n",
        "    '今日はとても眠いのでコーヒーを飲んで目を覚まします',\n",
        "    '新しい趣味を見つけたいです',\n",
        "]\n",
        "\n",
        "expected_emojis = [\n",
        "    '☀️',\n",
        "    '📚',\n",
        "    '😢',\n",
        "    '🎉',\n",
        "    '😫',\n",
        "    '✈️',\n",
        "    '😴',\n",
        "    '🎥',\n",
        "    '🍴',\n",
        "    '🔥',\n",
        "    '💼',\n",
        "    '🏋️‍♂️',\n",
        "    '😊',\n",
        "    '🎨',\n",
        "    '🐷',\n",
        "    '🎊',\n",
        "    '🚗',\n",
        "    '💼',\n",
        "    '🎤',\n",
        "    '💻',\n",
        "    '✈️',\n",
        "    '🧥',\n",
        "    '📖',\n",
        "    '☀️',\n",
        "    '👩‍🍳',\n",
        "    '🤔',\n",
        "    '🎬',\n",
        "    '🏋️‍♂️',\n",
        "    '💤',\n",
        "    '🏠',\n",
        "    '🎶',\n",
        "    '☔',\n",
        "    '🎢',\n",
        "    '😔',\n",
        "    '🔥',\n",
        "    '😴',\n",
        "    '🍴',\n",
        "    '😪',\n",
        "    '🎁',\n",
        "    '😢',\n",
        "    '🧺',\n",
        "    '😫',\n",
        "    '🏀',\n",
        "    '😄',\n",
        "    '🏠❄️',\n",
        "    '🍿',\n",
        "    '🎉',\n",
        "    '🍎',\n",
        "    '💬',\n",
        "    '🏠',\n",
        "    '📱',\n",
        "    '🥗',\n",
        "    '🎥',\n",
        "    '♨️',\n",
        "    '🌧️',\n",
        "    '🍽️',\n",
        "    '📚',\n",
        "    '👔💼',\n",
        "    '😴☕️',\n",
        "    '🎨',\n",
        "]\n",
        "\n",
        "test_translations = [\n",
        "    'Today is good weather, isn\\'t it?',\n",
        "    'There is an exam tomorrow.',\n",
        "    'There was a very sad incident yesterday.',\n",
        "    'I\\'m going to a party tonight.',\n",
        "    'I\\'ve been busy lately and stressed out.',\n",
        "    'I want to go on a trip.',\n",
        "    'I\\'m very tired today.',\n",
        "    'I\\'m going to see a movie with my friend.',\n",
        "    'I want to go to a new restaurant.',\n",
        "    'It\\'s very hot today.',\n",
        "    'I\\'m starting a new job from tomorrow.',\n",
        "    'I exercised with my friend.',\n",
        "    'I\\'m very happy today.',\n",
        "    'I started a new hobby.',\n",
        "    'I\\'ve been eating too much and gained weight.',\n",
        "    'I participated in a very fun event yesterday.',\n",
        "    'I\\'m planning to go on a trip with my family this weekend.',\n",
        "    'There is an important presentation tomorrow.',\n",
        "    'I went to karaoke with my friend.',\n",
        "    'Today was a very busy day.',\n",
        "    'I am planning to go on a trip from tomorrow.',\n",
        "    'I\\'ve been feeling cold lately and can\\'t let go of my coat.',\n",
        "    'I started reading a new book.',\n",
        "    'Today is very clear and feels good.',\n",
        "    'I cooked with my friend.',\n",
        "    'I\\'m very bored today and don\\'t know what to do.',\n",
        "    'I\\'m looking forward to the release of a new movie.',\n",
        "    'I\\'ve been inactive lately and my body feels heavy.',\n",
        "    'I slept early yesterday because I was very sleepy.',\n",
        "    'I plan to spend a relaxing day at home today.',\n",
        "    'I started listening to new music.',\n",
        "    'The weather forecast for tomorrow says it will rain.',\n",
        "    'I plan to go to an amusement park with my friends this weekend.',\n",
        "    'I\\'ve been very busy lately and haven\\'t had time to meet my friends.',\n",
        "    'Yesterday was very hot and it was hard to go outside.',\n",
        "    'I slept in today because I had a day off from work.',\n",
        "    'I want to try a new recipe.',\n",
        "    'I\\'ve been using my smartphone too much lately and my eyes are getting tired.',\n",
        "    'I will go to buy a present for my friend\\'s birthday tomorrow.',\n",
        "    'There was very sad news today.',\n",
        "    'I am planning to go on a picnic with my family today.',\n",
        "    'I have been busy with work lately and have been accumulating stress.',\n",
        "    'I want to try starting a new sport.',\n",
        "    'Yesterday was a lot of fun.',\n",
        "    'It is very cold today, so I am relaxing at home.',\n",
        "    'I am looking forward to watching a new movie.',\n",
        "    'I feel like doing something special today.',\n",
        "    'My diet has been unbalanced lately and my health is not good.',\n",
        "    'I got carried away talking with my friend yesterday.',\n",
        "    'I plan to spend a relaxing evening at home tonight.',\n",
        "    'I downloaded a new app.',\n",
        "    'I will start a new diet from tomorrow.',\n",
        "    'I have been watching a lot of movies lately.',\n",
        "    'I plan to go on a hot spring trip with my family this weekend.',\n",
        "    'It was raining yesterday, so I spent time at home.',\n",
        "    'I feel like eating something delicious today.',\n",
        "    'I haven\\'t had time to read books lately.',\n",
        "    'I have a very important presentation tomorrow.',\n",
        "    'I am very sleepy today, so I will drink coffee to wake up.',\n",
        "    'I want to find a new hobby.',\n",
        "]\n",
        "\n",
        "predicted_emojis = []\n",
        "for x in test_texts:predicted_emojis.append(predict_emoji(x))\n",
        "\n",
        "for c,x,y,z in zip(range(len(test_translations)),test_translations, expected_emojis, predicted_emojis):\n",
        "  print(c+1,x,y,z)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
